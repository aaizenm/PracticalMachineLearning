## Backgrownd

For this assignment, I build a predictive model to determine whether a particular form of exercise (barbell lifting) 
is performed correctly, using accelerometer data.

## Loading of the packages
The first part is the declaration of the package which will be used. 

Note : to be reproductible, I also set the seed value.

```{r}
library(caret)
library(randomForest)
library(Hmisc)
library(corrplot)

set.seed(1234)
```
## Reading of the Data

We begin by reading in the training and testing datasets, 
assigning missing values to entries that are currently 'NA' or blank.
(commands are commented to limit the output size. You can run it deleting the "#" ) 

```{r}
#getwd()
setwd("C:/PredictiveAnalysis/R")

train <- read.csv("./data/pml-training.csv", header = TRUE, na.strings = c("NA", ""))
test <- read.csv("./data/pml-testing.csv", header = TRUE, na.strings = c("NA", ""))

```
#dim(test)
#names(train)

dim(train)

## Cleaning the data

We see that the training set consists of 19622 observations of 160 variables
```{r}
sum(complete.cases(train))
#head(train)
```

The discussion here is choosing between:
Or discarding most of the observations but using more predictors 
Or discarding some predictors to keep most of the observations.
The conclusion is that more observations is better, while additional variables may or may not helping us.

```{r}
Columns in the orignal training and testing datasets that are mostly filled with missing values are removed. 

# colSums(is.na(train))   ## This will give us which columns have 0 values
# colSums(is.na(train))==0  ## will give us a set of FALSE and TRUE values 

newtrain<- train[,colSums(is.na(train))==0]
newtest<- test[,colSums(is.na(test))==0]
dim(newtrain)
```

As we see we have eliminated 2/3 of the columns. 60 columns.

#head(newtrain)

Some of the variables in this new data set do not come from accelerometer measurements and record experimental
setup or participants' data.

So the following variables will be take out as well: 
X, user_name, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp, new_window and num_window.

```{r}
todelete_cols <- grepl("X|user_name|new_window|num_window|raw_timestamp_part_1|raw_timestamp_part_2|cvtd_timestamp", colnames(newtrain))
finaltrain <- newtrai[, !todelete_cols]
finaltrain <- newtrain[, !todelete_cols]
#names(finaltrain)

Now we have 53 columns to work with.
```
dim(finaltrain)

## Spliting the data for validation purposes

Now we will split the final training dataset into a training (70% of the observations) and a validation 
(30% of the observations). 

This validation dataset will allow us to perform cross validation for developing/testing our model.


```{r}
inTrain = createDataPartition(y = finaltrain$classe, p = 0.7, list = FALSE)
finaltrain_train = finaltrain[inTrain, ]
finaltrain_valid = finaltrain[-inTrain, ]
```

## Understanding the Correlations between variables

We begin by looking at the correlations between the variables in our dataset. 
We may want to remove highly correlated predictors from our analysis and replace them with weighted 
combinations of predictors. 

The goal of your project is to predict the manner in which they did the exercise. 
This is the "classe" variable in the training set. 

```{r}
correlMatrix <- cor(finaltrain_train[, -53])
corrplot(correlMatrix, order = "FPC", method = "circle", type = "lower", tl.cex = 0.8,  tl.col = rgb(0, 0, 0))
```

This correlation plot shows the correlation between pairs of the predictors in our dataset. 
From a high-level perspective darker blue and darker red circles indicate high positive and high negative correlations, 
respectively. 

Nonetheless, there are a few pairs of variables that are highly correlated:

which(correlMatrix > 0.98 & correlMatrix != 1)
correlMatrix[which(correlMatrix > 0.98 & correlMatrix != 1)]
correlMatrix[which(correlMatrix < -0.98 )]


## Predictive Model - Method 1
Now , its time to pre-process the data using a principal component analysis, leaving out the column we want to predict ('classe'). 
After pre-processing, we use the 'predict' function to apply the pre-processing to the training and validation 
of the final training dataset.
```{r}
preProc <- preProcess(finaltrain_train[, -53], method = "pca", thresh = 0.99)
trainPC <- predict(preProc, finaltrain_train[, -53])
validationTestPC <- predict(preProc, finaltrain_valid[, -53])
```

Now, we train a model using a random forest approach on the smaller training dataset. 
Note that we chose to specify the use of a cross validation method when applying the random forest routine in the
'trainControl()' parameter. Worth mentioning that without specifying this, the default method (bootstrapping) 
would have been used. The bootstrapping method take a much longer time to complete, and we get the same level of 'accuracy'.

```{r}
modelFit <- train(finaltrain_train$classe ~ ., method = "rf", data = trainPC, trControl = trainControl(method = "cv", number = 4), importance = TRUE)
This took some minutes to process, (and therefore I'll try using a different method.)
modelFit
modelFit$finalModel
```

Now can review the relative importance of the resulting principal components of the trained model, 'modelFit':
```{r}
varImpPlot(modelFit$finalModel, sort = TRUE, type = 1, pch = 19, col = 1, cex = .6,  main = "Importance of the Individual Principal Components")
```
Now we show the The degree of importance is shown on the x-axisâ€“increasing from left to right. 

## Making my PC to work in parallel to cut the processing time
Not in the scope of this assignment, it will be convenient to add the following statements to accelerate the processing
time by using in parallel more than one CPUs
```{r}
install.packages("doParallel")
library(doParallel)
cl <- makeCluster(detectCores())
registerDoParallel(cl)
```

## Predictive Model - Method 2











